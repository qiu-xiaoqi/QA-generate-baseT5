{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 路径及参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./DuReaderQG/train.json\"\n",
    "test_path = \"./DuReaderQG/dev.json\"\n",
    "max_target_len = 32\n",
    "max_source_len = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载model和tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertTokenizerFast(name_or_path='./uer/t5-base-chinese-cluecorpussmall', vocab_size=21229, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),\n",
       " T5ForConditionalGeneration(\n",
       "   (shared): Embedding(21228, 768)\n",
       "   (encoder): T5Stack(\n",
       "     (embed_tokens): Embedding(21228, 768)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 12)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1-11): 11 x T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (decoder): T5Stack(\n",
       "     (embed_tokens): Embedding(21228, 768)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 12)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1-11): 11 x T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=768, out_features=21228, bias=False)\n",
       " ))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pertrained_path = './uer/t5-base-chinese-cluecorpussmall'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pertrained_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(pertrained_path)\n",
    "tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data = []\n",
    "        with open(data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_seq = (f\"问题：{item['question']}{tokenizer.sep_token}原文：{item['context']}\")\n",
    "        output_seq = f\"答案：{item['answer']}{tokenizer.eos_token}\"\n",
    "        return input_seq, output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('问题：仙剑奇侠传3第几集上天界[SEP]原文：第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。',\n",
       " '答案：第35集None')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = QADataset(train_path)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     test_train\u001b[38;5;241m.\u001b[39mappend(train_dataset[i]) \n\u001b[0;32m     31\u001b[0m train_test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mtrain_test_dataloader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    batched_data = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"decoder_input_ids\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "    for _, (input_seq, output_seq) in enumerate(batch):\n",
    "        # tokenize输入\n",
    "        inputs = tokenizer(text=input_seq, truncation=True, max_length=max_source_len, padding=True)\n",
    "        # tokenize输出,并将输出的ids作为inputs的label\n",
    "        output_ids = tokenizer.encode(text=output_seq, truncation=True, max_length=max_target_len)\n",
    "        decoder_input_ids = output_ids[:-2] # 去掉eos和[cls]\n",
    "        decoder_input_ids =  decoder_input_ids + [tokenizer.pad_token_id] * (max_target_len - len(decoder_input_ids)) # padding\n",
    "        \n",
    "        labels = output_ids[1: -1] # 去掉起始token和[cls]\n",
    "        labels = labels + [-100] * (max_target_len - len(labels))\n",
    "        \n",
    "        batched_data[\"input_ids\"].append(inputs[\"input_ids\"])\n",
    "        batched_data[\"attention_mask\"].append(inputs[\"attention_mask\"])\n",
    "        batched_data[\"decoder_input_ids\"].append(decoder_input_ids)\n",
    "        batched_data[\"labels\"].append(labels)\n",
    "        \n",
    "    for k, v in batched_data.items():\n",
    "        batched_data[k] = torch.tensor(np.array(v))\n",
    "    return batched_data\n",
    "test_train = []\n",
    "for i in range(5):\n",
    "    test_train.append(train_dataset[i]) \n",
    "\n",
    "train_test_dataloader = DataLoader(test_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "train_test_dataloader[0]                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
